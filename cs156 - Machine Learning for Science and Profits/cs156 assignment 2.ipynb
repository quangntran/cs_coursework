{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment attempts to build a model that aids making decision of how much one should apply for a loan. The data used is downloaded from [The Lending Club](https://www.lendingclub.com/info/download-data.action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "## Download and read data\n",
    "For each category (rejected or approved), we downloaded data from 2018 Q3 as those are the most recent ones. **Assumption:** More recent data reflect better the distribution of the current. \n",
    "\n",
    "Let us read in the data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (0,123,124,125,128,129,130,133,139,140,141) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>member_id</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>...</th>\n",
       "      <th>hardship_payoff_balance_amount</th>\n",
       "      <th>hardship_last_payment_amount</th>\n",
       "      <th>disbursement_method</th>\n",
       "      <th>debt_settlement_flag</th>\n",
       "      <th>debt_settlement_flag_date</th>\n",
       "      <th>settlement_status</th>\n",
       "      <th>settlement_date</th>\n",
       "      <th>settlement_amount</th>\n",
       "      <th>settlement_percentage</th>\n",
       "      <th>settlement_term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>6.67%</td>\n",
       "      <td>307.27</td>\n",
       "      <td>A</td>\n",
       "      <td>A2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DirectPay</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>18.94%</td>\n",
       "      <td>388.62</td>\n",
       "      <td>D</td>\n",
       "      <td>D2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8500.0</td>\n",
       "      <td>8500.0</td>\n",
       "      <td>8500.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>14.47%</td>\n",
       "      <td>292.46</td>\n",
       "      <td>C</td>\n",
       "      <td>C2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>6.67%</td>\n",
       "      <td>1075.43</td>\n",
       "      <td>A</td>\n",
       "      <td>A2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>28000.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>14.47%</td>\n",
       "      <td>658.36</td>\n",
       "      <td>C</td>\n",
       "      <td>C2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 145 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  member_id  loan_amnt  funded_amnt  funded_amnt_inv        term  \\\n",
       "0  NaN        NaN    10000.0      10000.0          10000.0   36 months   \n",
       "1  NaN        NaN    15000.0      15000.0          15000.0   60 months   \n",
       "2  NaN        NaN     8500.0       8500.0           8500.0   36 months   \n",
       "3  NaN        NaN    35000.0      35000.0          35000.0   36 months   \n",
       "4  NaN        NaN    28000.0      28000.0          28000.0   60 months   \n",
       "\n",
       "  int_rate  installment grade sub_grade       ...         \\\n",
       "0    6.67%       307.27     A        A2       ...          \n",
       "1   18.94%       388.62     D        D2       ...          \n",
       "2   14.47%       292.46     C        C2       ...          \n",
       "3    6.67%      1075.43     A        A2       ...          \n",
       "4   14.47%       658.36     C        C2       ...          \n",
       "\n",
       "  hardship_payoff_balance_amount hardship_last_payment_amount  \\\n",
       "0                            NaN                          NaN   \n",
       "1                            NaN                          NaN   \n",
       "2                            NaN                          NaN   \n",
       "3                            NaN                          NaN   \n",
       "4                            NaN                          NaN   \n",
       "\n",
       "  disbursement_method  debt_settlement_flag debt_settlement_flag_date  \\\n",
       "0           DirectPay                     N                       NaN   \n",
       "1                Cash                     N                       NaN   \n",
       "2                Cash                     N                       NaN   \n",
       "3                Cash                     N                       NaN   \n",
       "4                Cash                     N                       NaN   \n",
       "\n",
       "  settlement_status settlement_date settlement_amount  settlement_percentage  \\\n",
       "0               NaN             NaN               NaN                    NaN   \n",
       "1               NaN             NaN               NaN                    NaN   \n",
       "2               NaN             NaN               NaN                    NaN   \n",
       "3               NaN             NaN               NaN                    NaN   \n",
       "4               NaN             NaN               NaN                    NaN   \n",
       "\n",
       "   settlement_term  \n",
       "0              NaN  \n",
       "1              NaN  \n",
       "2              NaN  \n",
       "3              NaN  \n",
       "4              NaN  \n",
       "\n",
       "[5 rows x 145 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_app = pd.read_csv(\"./data2/LoanStats_2018Q3.csv\", skiprows=[0], sep=',',na_values=['NULL', ''])\n",
    "df_app.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amount Requested</th>\n",
       "      <th>Application Date</th>\n",
       "      <th>Loan Title</th>\n",
       "      <th>Risk_Score</th>\n",
       "      <th>Debt-To-Income Ratio</th>\n",
       "      <th>Zip Code</th>\n",
       "      <th>State</th>\n",
       "      <th>Employment Length</th>\n",
       "      <th>Policy Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>2018-07-01</td>\n",
       "      <td>Debt consolidation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100%</td>\n",
       "      <td>925xx</td>\n",
       "      <td>CA</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40000.0</td>\n",
       "      <td>2018-07-01</td>\n",
       "      <td>Major purchase</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.45%</td>\n",
       "      <td>335xx</td>\n",
       "      <td>FL</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16000.0</td>\n",
       "      <td>2018-07-01</td>\n",
       "      <td>Debt consolidation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.93%</td>\n",
       "      <td>156xx</td>\n",
       "      <td>PA</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40000.0</td>\n",
       "      <td>2018-07-01</td>\n",
       "      <td>Debt consolidation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.87%</td>\n",
       "      <td>957xx</td>\n",
       "      <td>CA</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300000.0</td>\n",
       "      <td>2018-07-01</td>\n",
       "      <td>Business Loan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1%</td>\n",
       "      <td>258xx</td>\n",
       "      <td>TN</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Amount Requested Application Date          Loan Title  Risk_Score  \\\n",
       "0            3000.0       2018-07-01  Debt consolidation         NaN   \n",
       "1           40000.0       2018-07-01      Major purchase         NaN   \n",
       "2           16000.0       2018-07-01  Debt consolidation         NaN   \n",
       "3           40000.0       2018-07-01  Debt consolidation         NaN   \n",
       "4          300000.0       2018-07-01       Business Loan         NaN   \n",
       "\n",
       "  Debt-To-Income Ratio Zip Code State Employment Length  Policy Code  \n",
       "0                 100%    925xx    CA          < 1 year            0  \n",
       "1                7.45%    335xx    FL          < 1 year            0  \n",
       "2               34.93%    156xx    PA          < 1 year            0  \n",
       "3               27.87%    957xx    CA          < 1 year            0  \n",
       "4                  -1%    258xx    TN          < 1 year            0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rej = pd.read_csv(\"./data2/RejectStats_2018Q3.csv\", skiprows=[0], sep=',',na_values=['NULL', ''])\n",
    "df_rej.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The road map to solve the problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now take a look at the column of funded amount (`funded_amnt`) and compare it with the requested amount (`loan_amnt`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(df_app['loan_amnt'] != df_app['funded_amnt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5601110799088896e-05"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(df_app['loan_amnt'] != df_app['funded_amnt'])/df_app.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are only 2 instances where, if the request is approved, the requested amount differs from the actual amount funded. This accounts for a negligible portion of our huge data set ($1.6\\times10^{-5}$). We can safely assume that **the funded amount is equal to the requested amount**, given that the project is accepted. Hence, our overarching task of \"predicting the largest loan amount that will be successfully funded for given individual\" **boils down to the task of classifying whether an individual is approved or rejected**, as once the individual is accepted, the amount of fund is always equal to the amount requested (i.e., no need for regression for the amount funded). It is also worth pointing out that the cases where the requested amount does not equal the funded amount are those with missing data for those values, not because the funded amount is higher or lower than the requested one.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128194</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128195</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loan_amnt  funded_amnt\n",
       "128194        NaN          NaN\n",
       "128195        NaN          NaN"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = df_app['loan_amnt'] != df_app['funded_amnt']\n",
    "df_app[pos][['loan_amnt','funded_amnt']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find common features\n",
    "\n",
    "We find features shared by both types of data set. Below is the summary.\n",
    "\n",
    "Reject | Approve\n",
    "--- | --- \n",
    "Amount Requested | loan_amnt\n",
    "Loan Title | title\n",
    "Debt-To-Income Ratio | dti\n",
    "Zip Code | zip_code\n",
    "State | addr_state\n",
    "Employment Length | emp_length\n",
    "Policy Code | policy_code\n",
    "\n",
    "Therefore, we will drop all the columns that are not tabulated above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approve shape: (128196, 7)\n",
      "Reject shape: (2585245, 7)\n"
     ]
    }
   ],
   "source": [
    "common_feat_app = ['loan_amnt', 'title', 'dti', 'zip_code', \n",
    "                   'addr_state', 'emp_length','policy_code']\n",
    "common_feat_rej = ['Amount Requested', 'Loan Title', \n",
    "                   'Debt-To-Income Ratio', 'Zip Code',\n",
    "                  'State', 'Employment Length', 'Policy Code']\n",
    "\n",
    "# drop unshared columns\n",
    "df_app = df_app[common_feat_app]\n",
    "df_rej = df_rej[common_feat_rej]\n",
    "# shapes of new sets\n",
    "print('Approve shape:', df_app.shape)\n",
    "print('Reject shape:', df_rej.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check missing values\n",
    "\n",
    "We will check to see if there are missing values (and if so, how many) in each of the common features listed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approve data\n",
      "--------------------------------------------------\n",
      "2 loan_amnt missing, accounting for 0.00\n",
      "2 title missing, accounting for 0.00\n",
      "309 dti missing, accounting for 0.00\n",
      "2 zip_code missing, accounting for 0.00\n",
      "2 addr_state missing, accounting for 0.00\n",
      "10389 emp_length missing, accounting for 0.08\n",
      "2 policy_code missing, accounting for 0.00\n",
      "\n",
      "Reject data\n",
      "--------------------------------------------------\n",
      "0 Amount Requested missing, accounting for 0.00\n",
      "0 Loan Title missing, accounting for 0.00\n",
      "0 Debt-To-Income Ratio missing, accounting for 0.00\n",
      "0 Zip Code missing, accounting for 0.00\n",
      "0 State missing, accounting for 0.00\n",
      "104087 Employment Length missing, accounting for 0.04\n",
      "0 Policy Code missing, accounting for 0.00\n"
     ]
    }
   ],
   "source": [
    "def check_na(df, feature_list):\n",
    "    for ft in feature_list:\n",
    "        tot_missing = np.sum(pd.isna(df[ft]))\n",
    "        perc_missing = tot_missing/len(df)\n",
    "        print('%d %s missing, accounting for %.2f'%\n",
    "             (tot_missing, ft, perc_missing))\n",
    "\n",
    "# for approve data\n",
    "print('Approve data')\n",
    "print('-'*50)\n",
    "check_na(df_app, common_feat_app)\n",
    "print('\\nReject data')\n",
    "print('-'*50)\n",
    "check_na(df_rej, common_feat_rej)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that except for the length of employment, all the features have a negligible number of missing values (accounting for virtually zero percent of the data sets). That said, we can discard any observations that have values for these features missing. A more concerning case is with the employment length, where $\\approx 8\\%$ and $\\approx 4\\%$ of the approve and reject data, repecstively, have missing values for this feature. What is worrying about disregarding these cases is that we don't know why they are missing. If we knew that they are missing at random, then discarding them would pose no considerable effect, especially given the small percentage the missing cases represent in the data sets. However, our analysis would be biased if there is a systematic difference between the missing cases and the other ones. To this end, we will impute the missing `emp_length` with its mean.\n",
    "\n",
    "\n",
    "### Drop missing values (except for `emp_length` and `Employment Length`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df_app after dropping: (127887, 7)\n",
      "Shape of df_rej after dropping: (2585245, 7)\n"
     ]
    }
   ],
   "source": [
    "# drop df_app\n",
    "df_app_dropped = df_app.dropna(subset=['loan_amnt',\n",
    "                                      'title',\n",
    "                                      'dti',\n",
    "                                      'zip_code',\n",
    "                                      'addr_state',\n",
    "                                      'policy_code'])\n",
    "# drop df_rej\n",
    "df_rej_dropped = df_rej # nothing to drop\n",
    "print('Shape of df_app after dropping:', df_app_dropped.shape)\n",
    "print('Shape of df_rej after dropping:', df_rej_dropped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>title</th>\n",
       "      <th>dti</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>addr_state</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>policy_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>Debt consolidation</td>\n",
       "      <td>100%</td>\n",
       "      <td>925xx</td>\n",
       "      <td>CA</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40000.0</td>\n",
       "      <td>Major purchase</td>\n",
       "      <td>7.45%</td>\n",
       "      <td>335xx</td>\n",
       "      <td>FL</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   loan_amnt               title    dti zip_code addr_state emp_length  \\\n",
       "0     3000.0  Debt consolidation   100%    925xx         CA   < 1 year   \n",
       "1    40000.0      Major purchase  7.45%    335xx         FL   < 1 year   \n",
       "\n",
       "   policy_code  \n",
       "0            0  \n",
       "1            0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change the column names of the reject data set to\n",
    "# match with those of the approve data set\n",
    "df_rej_dropped.columns = df_app_dropped.columns\n",
    "df_rej.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare values of two data sets\n",
    "\n",
    "Now we check values of each feature in each data set to see if they are represented by the same coding. \n",
    "\n",
    "**1. loan_amnt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "print(df_app_dropped.loan_amnt.dtype)\n",
    "print(df_rej_dropped.loan_amnt.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. title**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n",
      "object\n",
      "['Credit card refinancing' 'Debt consolidation' 'Home improvement'\n",
      " 'Medical expenses' 'Home buying' 'Major purchase' 'Other' 'Vacation'\n",
      " 'Moving and relocation' 'Car financing' 'Business' 'Green loan']\n",
      "['Debt consolidation' 'Major purchase' 'Business Loan'\n",
      " 'Credit card refinancing' 'Other' 'Medical expenses' 'Car financing'\n",
      " 'Green loan' 'Moving and relocation' 'Home buying' 'Home improvement'\n",
      " 'Business' 'Vacation' 'home_improvement']\n"
     ]
    }
   ],
   "source": [
    "print(df_app_dropped.title.dtype)\n",
    "print(df_rej_dropped.title.dtype)\n",
    "print(df_app_dropped.title.unique())\n",
    "print(df_rej_dropped.title.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that all the categories presented in the approve data set is also mentioned in the rejected data set. The only differences are that some are capitalized and some are not (for example, 'other' and 'Other', some comtain words with blank spaces while other contain a sub bar (e.g, 'Home improvement' and 'home_imrovement'). For this reason, we lowercase all the categories and replace 'home_improvement' with 'home improvement'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "df_app_dropped['title'] = df_app_dropped['title'].str.lower()\n",
    "df_rej_dropped['title'] = df_rej_dropped['title'].str.lower()\n",
    "df_app_dropped['title'] = df_app_dropped['title'].replace('home_improvement',\n",
    "                                                         'home improvement')\n",
    "df_rej_dropped['title'] = df_rej_dropped['title'].replace('home_improvement',\n",
    "                                                         'home improvement')\n",
    "df_rej_dropped['title'] = df_rej_dropped['title'].replace('business loan',\n",
    "                                                         'business')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. dti**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "df_rej_dropped['dti'] = df_rej_dropped['dti'].str.replace('%','')\n",
    "df_rej_dropped['dti'] = pd.to_numeric(df_rej_dropped['dti'])\n",
    "print(df_app_dropped.dti.dtype)\n",
    "print(df_rej_dropped.dti.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. zip_code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n",
      "object\n"
     ]
    }
   ],
   "source": [
    "print(df_app_dropped.zip_code.dtype)\n",
    "print(df_rej_dropped.zip_code.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. addr_state **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n",
      "object\n",
      "['NJ' 'VA' 'AZ' 'NH' 'CA' 'WI' 'FL' 'TX' 'MA' 'NC' 'MD' 'MN' 'CO' 'NY'\n",
      " 'MI' 'MO' 'WV' 'KY' 'IN' 'CT' 'LA' 'ME' 'IL' 'OH' 'GA' 'TN' 'NV' 'OK'\n",
      " 'PA' 'ND' 'AR' 'UT' 'OR' 'HI' 'WA' 'SC' 'MS' 'AL' 'NE' 'DE' 'DC' 'RI'\n",
      " 'KS' 'ID' 'MT' 'NM' 'VT' 'SD' 'WY' 'AK']\n",
      "['CA' 'FL' 'PA' 'TN' 'AZ' 'HI' 'OR' 'KY' 'MT' 'ID' 'LA' 'IL' 'WI' 'MI'\n",
      " 'WA' 'NY' 'AR' 'OH' 'TX' 'NV' 'NC' 'SC' 'MA' 'MO' 'NJ' 'CO' 'VA' 'MS'\n",
      " 'OK' 'NM' 'WV' 'VT' 'MN' 'GA' 'AL' 'UT' 'KS' 'SD' 'DC' 'IN' 'NE' 'ND'\n",
      " 'MD' 'ME' 'RI' 'NH' 'WY' 'CT' 'DE' 'AK' 'IA']\n"
     ]
    }
   ],
   "source": [
    "print(df_app_dropped.addr_state.dtype)\n",
    "print(df_rej_dropped.addr_state.dtype)\n",
    "print(df_app_dropped.addr_state.unique())\n",
    "print(df_rej_dropped.addr_state.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. policy_code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "int64\n",
      "[1.]\n",
      "[0 2]\n",
      "0.994885204303654\n"
     ]
    }
   ],
   "source": [
    "print(df_app_dropped.policy_code.dtype)\n",
    "print(df_rej_dropped.policy_code.dtype)\n",
    "print(df_app_dropped.policy_code.unique())\n",
    "print(df_rej_dropped.policy_code.unique())\n",
    "print(np.sum(df_rej_dropped['policy_code'] == 0)/len(df_rej_dropped))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the dictionary shows that the policy code can only take values of 1 or 2, the 0 values are present predominantly in the reject data set. Due to this conflict of information, we decided to exlude this feature from the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_app_dropped = df_app_dropped.drop(['policy_code'], axis=1)\n",
    "df_rej_dropped = df_rej_dropped.drop(['policy_code'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. emp_length**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n",
      "object\n",
      "['10+ years' '2 years' '4 years' '< 1 year' '8 years' nan '6 years'\n",
      " '3 years' '5 years' '7 years' '1 year' '9 years']\n",
      "['< 1 year' nan '2 years' '1 year' '5 years' '9 years' '10+ years'\n",
      " '8 years' '3 years' '4 years' '7 years' '6 years']\n"
     ]
    }
   ],
   "source": [
    "print(df_app_dropped.emp_length.dtype)\n",
    "print(df_rej_dropped.emp_length.dtype)\n",
    "print(df_app_dropped.emp_length.unique())\n",
    "print(df_rej_dropped.emp_length.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there is a natural order of the employment length (e.g., 3 years are longer than 1 year), we will encode this feature as a numerical one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approve data\n",
    "df_app_dropped['emp_length'] = df_app_dropped['emp_length'].replace('< 1 year',0)\n",
    "df_app_dropped['emp_length'] = df_app_dropped['emp_length'].replace('1 year',1)\n",
    "df_app_dropped['emp_length'] = df_app_dropped['emp_length'].replace('10+ years',10)\n",
    "for i in range(2,10):\n",
    "    txt = str(i) + ' years'\n",
    "    df_app_dropped['emp_length'] = df_app_dropped['emp_length'].replace(txt,\n",
    "                                                         i)\n",
    "# reject data\n",
    "df_rej_dropped['emp_length'] = df_rej_dropped['emp_length'].replace('< 1 year',\n",
    "                                                         0)\n",
    "df_rej_dropped['emp_length'] = df_rej_dropped['emp_length'].replace('1 year',\n",
    "                                                         1)\n",
    "df_rej_dropped['emp_length'] = df_rej_dropped['emp_length'].replace('10+ years',\n",
    "                                                         10)\n",
    "for i in range(2,10):\n",
    "    txt = str(i) + ' years'\n",
    "    df_rej_dropped['emp_length'] = df_rej_dropped['emp_length'].replace(txt,\n",
    "                                                         i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_app_dropped['rejected'] = 0\n",
    "df_rej_dropped['rejected'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df_app_dropped, df_rej_dropped]\n",
    "df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>title</th>\n",
       "      <th>dti</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>addr_state</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>credit card refinancing</td>\n",
       "      <td>19.22</td>\n",
       "      <td>070xx</td>\n",
       "      <td>NJ</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15000.0</td>\n",
       "      <td>debt consolidation</td>\n",
       "      <td>25.60</td>\n",
       "      <td>245xx</td>\n",
       "      <td>VA</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8500.0</td>\n",
       "      <td>debt consolidation</td>\n",
       "      <td>6.33</td>\n",
       "      <td>852xx</td>\n",
       "      <td>AZ</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>debt consolidation</td>\n",
       "      <td>17.07</td>\n",
       "      <td>030xx</td>\n",
       "      <td>NH</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28000.0</td>\n",
       "      <td>debt consolidation</td>\n",
       "      <td>13.24</td>\n",
       "      <td>958xx</td>\n",
       "      <td>CA</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   loan_amnt                    title    dti zip_code addr_state  emp_length  \\\n",
       "0    10000.0  credit card refinancing  19.22    070xx         NJ        10.0   \n",
       "1    15000.0       debt consolidation  25.60    245xx         VA        10.0   \n",
       "2     8500.0       debt consolidation   6.33    852xx         AZ         2.0   \n",
       "3    35000.0       debt consolidation  17.07    030xx         NH         4.0   \n",
       "4    28000.0       debt consolidation  13.24    958xx         CA        10.0   \n",
       "\n",
       "   rejected  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute `emp_length`\n",
    "\n",
    "To impute the feature, we will use the mean `of emp_length`. We first split the data into training and testing (this is for later classification). Then we use the mean of the `emp_length` in the training data set to fill in the missing values in both training and testing sets. \n",
    "\n",
    "#### Train-val-test split\n",
    "\n",
    "Before we do the splitting, we will one hot encode all the categoricals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>title</th>\n",
       "      <th>dti</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>addr_state</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>credit card refinancing</td>\n",
       "      <td>19.22</td>\n",
       "      <td>070xx</td>\n",
       "      <td>NJ</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15000.0</td>\n",
       "      <td>debt consolidation</td>\n",
       "      <td>25.60</td>\n",
       "      <td>245xx</td>\n",
       "      <td>VA</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8500.0</td>\n",
       "      <td>debt consolidation</td>\n",
       "      <td>6.33</td>\n",
       "      <td>852xx</td>\n",
       "      <td>AZ</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>debt consolidation</td>\n",
       "      <td>17.07</td>\n",
       "      <td>030xx</td>\n",
       "      <td>NH</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28000.0</td>\n",
       "      <td>debt consolidation</td>\n",
       "      <td>13.24</td>\n",
       "      <td>958xx</td>\n",
       "      <td>CA</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   loan_amnt                    title    dti zip_code addr_state  emp_length  \\\n",
       "0    10000.0  credit card refinancing  19.22    070xx         NJ        10.0   \n",
       "1    15000.0       debt consolidation  25.60    245xx         VA        10.0   \n",
       "2     8500.0       debt consolidation   6.33    852xx         AZ         2.0   \n",
       "3    35000.0       debt consolidation  17.07    030xx         NH         4.0   \n",
       "4    28000.0       debt consolidation  13.24    958xx         CA        10.0   \n",
       "\n",
       "   rejected  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = pd.get_dummies(df,prefix=['title', 'zip_code', 'addr_state'],\n",
    "              columns=['title', 'zip_code', 'addr_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe train and test\n",
    "train, test = train_test_split(df_encoded, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:5430: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    }
   ],
   "source": [
    "mean_emp_length = train['emp_length'].mean()\n",
    "train['emp_length'].fillna((mean_emp_length), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:5430: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    }
   ],
   "source": [
    "test['emp_length'].fillna((mean_emp_length), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I intend to use regularization in the logistic regression model, feature scaling is necessary. Currently the `loan_amnt` and `dti` differ each other by several orders of magnitude. We will scale both of them by substracting the mean and divide them by their respective standard deviations. To do this, (**overfitting**) we use the means and the standard deviations from the training set to apply to both sets, as this prevents info leakage (as opposed to using means and standard deviations of both training and testing sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_loan_amnt = train['loan_amnt'].mean()\n",
    "sd_loan_amnt = train['loan_amnt'].std()\n",
    "mean_dti = train['dti'].mean()\n",
    "sd_dti = train['dti'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "train['loan_amnt'] = (train['loan_amnt'] - mean_loan_amnt)/sd_loan_amnt\n",
    "train['dti'] = (train['dti'] - mean_dti)/sd_dti\n",
    "test['loan_amnt'] = (test['loan_amnt'] - mean_loan_amnt)/sd_loan_amnt\n",
    "test['dti'] = (test['dti'] - mean_dti)/sd_dti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(['rejected'], axis=1)\n",
    "y_train = train['rejected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test.drop(['rejected'], axis=1)\n",
    "y_test = test['rejected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, \n",
    "                                                  y_train, \n",
    "                                                  test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsample data (#modelmetric)\n",
    "\n",
    "We have established that we will solve the problem by classification. If we think ahead how we will evaluate such a model, an appropriate metric to use is the accuracy. This is because there is no obvious reason for why we would try to avoid one error type more than the other. We assume that a positive case is a rejected case. Type I error (false alarm) occurs when we predict a case that is approved as rejected. Sounds bad! Type II error (miss detection) happens when a case is rejected but we predict it as approved. Sounds equally bad! In decision making, the former error may make the applicant diffident and not apply for the loan and therefore miss the opportunity. Type II error may makes one overconfident and submit an application that will fail them. Neither one is an obvious worse case, so we will opt for **accuracy** as our metric to evaluate the model's performance. However, we can easily achieve a 95% accuracy by predicting every application that comes in the door as \"rejected\", because that is how our data distribution currently looks like (see the cell below), and our machine is worthless. To avoid this, we will downsample the class rejected (1) to make even proportions of the two classes. We only do this on the training set and not the validation set or test set, because:\n",
    "* If we downsampled on the test set, we would be assuming and forcing some kind of distribution on the unseen data, which makes our machine unrealistic. \n",
    "* If we downsampled on the val set, the validation error rate does not give a good estimate of the test error rate as the distribution of the data would be vastly different.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9530022967005375\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(y_train)/len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "y_train_reset_index = y_train.reset_index(drop=True)\n",
    "X_train_reset_index = X_train.reset_index(drop=True)\n",
    "indices = np.where(y_train_reset_index == 1)[0] \n",
    "rng = np.random.RandomState(13)\n",
    "rng.shuffle(indices)\n",
    "n_neg = (y_train_reset_index == 0).sum()\n",
    "y_train_downsampled = y_train_reset_index.drop(y_train_reset_index.index[indices[n_neg:]])\n",
    "X_train_downsampled = X_train_reset_index.drop(X_train_reset_index.index[indices[n_neg:]])\n",
    "print(np.sum(y_train_downsampled)/len(y_train_downsampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The code above is cut off. The last 3 lines read:*\n",
    "\n",
    "`y_train_downsampled = y_train_reset_index.drop(y_train_reset_index.index[indices[n_neg:]])`\n",
    "\n",
    "`X_train_downsampled = X_train_reset_index.drop(X_train_reset_index.index[indices[n_neg:]])`\n",
    "\n",
    "`print(np.sum(y_train_downsampled)/len(y_train_downsampled))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some parameters to tune: \n",
    "* **penalty:** This is the regularization scheme to prevent the coefficients from being exploded. There are two schemes available in `sklearn`, L1 and L2. L1 is the sum of absolutes, while L2 is the sum of squares. \n",
    "* **C**: this is the inverse of regularization strength, the term that the regularization is multiplied by. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 finished\n",
      "2 finished\n",
      "3 finished\n",
      "4 finished\n",
      "5 finished\n",
      "6 finished\n",
      "7 finished\n",
      "8 finished\n",
      "9 finished\n",
      "10 finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "best_params = {'penalty': None, 'C': None}\n",
    "best_acc = -1\n",
    "i = 0 \n",
    "for penalty in ['l1', 'l2']:\n",
    "    for C in [1/1e-3, 1/1e-1, 1, 1/1e1, 1/1e3]:\n",
    "        clf = LogisticRegression(penalty=penalty,\n",
    "                                 C=C)\n",
    "        clf.fit(X_train_downsampled, y_train_downsampled)\n",
    "        preds = clf.predict(X_val)\n",
    "        acc = np.sum(preds==y_val)/len(y_val)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_params['penalty'] = penalty\n",
    "            best_params['C'] = C\n",
    "        i += 1\n",
    "        print('%d finished'%i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'penalty': 'l2', 'C': 0.1}\n",
      "best val accuracy: 0.9495578217972315\n"
     ]
    }
   ],
   "source": [
    "print('best params:', best_params)\n",
    "print('best val accuracy:', best_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy: 0.949685511410232\n"
     ]
    }
   ],
   "source": [
    "penalty, C = best_params.values()\n",
    "clf = LogisticRegression(penalty=penalty, C=C)\n",
    "clf.fit(X_train_downsampled, y_train_downsampled)\n",
    "preds = clf.predict(X_test)\n",
    "acc = np.sum(preds==y_test)/len(y_test)\n",
    "print('Testing accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing accuracy is fairly high despite the very differeny distributions of data in the training set and testing set. (In training set, we downsample the class rejected so there is a 50/50 split between two classes.) The testing accuracy is also very close to the best validation accuracy, which means our model does not overfit. \n",
    "\n",
    "There are a few limitations to the model:\n",
    "\n",
    "1. There is a cap to the latest time the data is relevant. That means current decisions based on this model risk being inaccurate due to any possible time-dependent variables.\n",
    "\n",
    "2. When new/more recent data come in, we have to build the model from scratch. With logistic regression model, there is no scheme to augment new data to the learned model. \n",
    "\n",
    "3. Imputation is done by taking the means. This may or may not be appropriate. For example, if the data is missing not at random and the reason it is missing is that because people are unemployed (i.e., employment length = 0) so they do not report it. Our way of taking a mean would systematically overestimate the employment length. \n",
    "\n",
    "4. We used downsampling. This essentially assumes a uniform disitribution of the two classes. A classifer that does not need this assumption and still maintains good accuracy on testing data set may yield better results.\n",
    "\n",
    "How to use the model to decide how much to apply: The coefficient for `loan_amnt` is $-0.077<0$. This means the model predicts that the higher we apply for, the lower the chance of getting accepted, assuming that all else are equal. The probability that an application will be accepted as a function of the amount requested takes the form:\n",
    "$$p(X_1)=\\frac{1}{1+e^{A-0.077X_1}}$$\n",
    "where A is the sum of other variables weighted by their coefficients.\n",
    "As the above function is continuous and decreasing monotonically as $X_1$ increases, we then can let X_1 varies increasingly until the probability hits a threshold/risk at which we can tolerate. We see that there is a tradeoff between the maximum loan funded and the chance of being accepted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
