{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the entire MNIST digit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import model_selection\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from scipy import stats\n",
    "from sklearn.svm import SVC\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install python-mnist\n",
    "from mnist import MNIST\n",
    "\n",
    "mndata = MNIST('samples')\n",
    "X_train, y_train = mndata.load_training()\n",
    "\n",
    "X_test, y_test = mndata.load_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose two digit classes (e.g 7s and 3s) from the training data, and plot some of the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prune training and testing data sets so they contain only\n",
    "# 0 and 1\n",
    "def get_data(X, y, digit_list):\n",
    "    mask = []\n",
    "    for label in y:\n",
    "        to_append = False\n",
    "        for digit in digit_list:\n",
    "            if label == digit:\n",
    "                to_append = True\n",
    "        mask.append(to_append)\n",
    "    return X[mask,:], y[mask]\n",
    "X_train01, y_train01 = get_data(X_train, y_train, [0,1])\n",
    "X_test01, y_test01 = get_data(X_test, y_test, [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training features: (9498, 784)\n",
      "Shape of training labels: (9498,)\n",
      "Shape of val features: (3167, 784)\n",
      "Shape of val labels: (3167,)\n",
      "Shape of testing features: (2115, 784)\n",
      "Shape of testing labels: (2115,)\n"
     ]
    }
   ],
   "source": [
    "# validation set split\n",
    "X_train01, X_val01, y_train01, y_val01 = model_selection.train_test_split(X_train01, \n",
    "                                                                  y_train01, \n",
    "                                                                  test_size=.25)\n",
    "\n",
    "print('Shape of training features:', X_train01.shape)\n",
    "print('Shape of training labels:', y_train01.shape)\n",
    "print('Shape of val features:', X_val01.shape)\n",
    "print('Shape of val labels:', y_val01.shape)\n",
    "print('Shape of testing features:', X_test01.shape)\n",
    "print('Shape of testing labels:', y_test01.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Plotting some examples ...\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting some examples\n",
    "print('\\nPlotting some examples ...')\n",
    "images_and_labels = list(zip(X_train01,y_train01))\n",
    "for index, (image, label) in enumerate(images_and_labels[:8]):\n",
    "    plt.subplot(2, 4, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(np.reshape(image,(28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Label: %i' % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the RBF kernel involves exponentials, and exponentials with large number can be unstable, I normalized the data. Because a lot of features in the original data have the same value (i.e., 0), the scheme of subtracting the mean and divided by the standard deviation fails because the std of those features are 0 and we cannot divide by 0. Therefore, we ignore these features by setting the `nan` in the resulted normalized data to 0. We can do this because features which are all 0 contribute nothing to the model and the decision. This can be done by `np.nan_to_num`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in true_divide\n",
      "  after removing the cwd from sys.path.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  \"\"\"\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \"\"\"\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Normalizing data\n",
    "mean_train = np.mean(X_train01,axis=0)\n",
    "std_train = np.std(X_train01,axis=0)\n",
    "X_train01_normalized = np.nan_to_num((X_train01 - mean_train)/std_train)\n",
    "X_val01_normalized = np.nan_to_num((X_val01 - mean_train)/std_train)\n",
    "X_test01_normalized = np.nan_to_num((X_test01 - mean_train)/std_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a support vector classifier using each of the following kernels:\n",
    "\n",
    "(**#overfitting**) In all the three sections corresponding to the three kernels, I used the validation set approach for tuning the parameters (as opposed to use cross validation). This is because our data is large enough. The reason we would want to use cross validation is usually when our training data is small and designating a fixed part of the data as a validation set would make the remaining training data even smaller and model trained on small data may not be generalized well. \n",
    "\n",
    "## Linear\n",
    "\n",
    "We will be tuning C (in this kernel and the other), the extent to which we penalize misclassifications when the data is linearly inseparable (in soft-margin SVM), which could happen even with using kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:36: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 1/7\n",
      "Finished 2/7\n",
      "Finished 3/7\n",
      "Finished 4/7\n",
      "Finished 5/7\n",
      "Finished 6/7\n",
      "Finished 7/7\n"
     ]
    }
   ],
   "source": [
    "# validation\n",
    "kernel = 'linear'\n",
    "random_state = 1\n",
    "best_acc = -1\n",
    "best_params = -1\n",
    "i = 0 \n",
    "for C in [1e-3,1e-2,1e-1,1,10,100,1000]:\n",
    "        clf = SVC(C=C, kernel=kernel, random_state=random_state)\n",
    "        clf.fit(X_train01_normalized, y_train01)\n",
    "        preds = clf.predict(X_val01_normalized)\n",
    "        acc = accuracy_score(y_val01, preds)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_params = C\n",
    "        i += 1\n",
    "        print('Finished %d/7'%(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.9993684875276286\n",
      "Best C: 0.1\n"
     ]
    }
   ],
   "source": [
    "print('Best accuracy:', best_acc)\n",
    "best_C = best_params\n",
    "print('Best C:', best_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_C = .1\n",
    "kernel='linear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time for Linear Kernel: 0.8229702663421631 s\n"
     ]
    }
   ],
   "source": [
    "# Get training time on the best params\n",
    "running_time = 0\n",
    "for _ in range(25):\n",
    "    start = time.time()\n",
    "    clf_lnr = SVC(C=best_C, kernel=kernel)\n",
    "    clf_lnr.fit(X_train01_normalized, y_train01)\n",
    "    running_time = running_time + (time.time()-start)\n",
    "avg_running_time_lnr = running_time/25\n",
    "print('Running time for Linear Kernel:', avg_running_time_lnr, 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poly\n",
    "\n",
    "We additionally tune `degree`, the degree $d$ of the polynomial kernel function:\n",
    "$$k(x_i,x_{i'})=(1+\\sum_{j=1}^{p}x_{ij}x_{i'j})^d$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:36: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 1/25\n",
      "Finished 2/25\n",
      "Finished 3/25\n",
      "Finished 4/25\n",
      "Finished 5/25\n",
      "Finished 6/25\n",
      "Finished 7/25\n",
      "Finished 8/25\n",
      "Finished 9/25\n",
      "Finished 10/25\n",
      "Finished 11/25\n",
      "Finished 12/25\n",
      "Finished 13/25\n",
      "Finished 14/25\n",
      "Finished 15/25\n",
      "Finished 16/25\n",
      "Finished 17/25\n",
      "Finished 18/25\n",
      "Finished 19/25\n",
      "Finished 20/25\n",
      "Finished 21/25\n",
      "Finished 22/25\n",
      "Finished 23/25\n",
      "Finished 24/25\n",
      "Finished 25/25\n"
     ]
    }
   ],
   "source": [
    "# validation\n",
    "kernel = 'poly'\n",
    "random_state = 1\n",
    "best_acc = -1\n",
    "best_params = [-1,-1]  \n",
    "i = 0 \n",
    "for C in [1e-3, 1e-1, 1, 100, 1000]:\n",
    "    for degree in range(2,11,2):\n",
    "        clf = SVC(C=C, kernel=kernel, degree=degree, random_state=random_state)\n",
    "        clf.fit(X_train01_normalized, y_train01)\n",
    "        preds = clf.predict(X_val01_normalized)\n",
    "        acc = accuracy_score(y_val01, preds)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_params = [C, degree]\n",
    "        i += 1\n",
    "        print('Finished %d/25'%(i))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.9968424376381434\n",
      "Best C: 1\n",
      "Best degree: 2\n"
     ]
    }
   ],
   "source": [
    "print('Best accuracy:', best_acc)\n",
    "best_C, best_degree = best_params[0], best_params[1]\n",
    "print('Best C:', best_C)\n",
    "print('Best degree:', best_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time for Poly Kernel: 7.8144989013671875 s\n"
     ]
    }
   ],
   "source": [
    "# Get training time on the best params\n",
    "running_time = 0\n",
    "for _ in range(5):\n",
    "    start = time.time()\n",
    "    clf_poly = SVC(C=best_C, kernel=kernel, degree=best_degree)\n",
    "    clf_poly.fit(X_train01_normalized, y_train01)\n",
    "    running_time = running_time + (time.time()-start)\n",
    "avg_running_time_poly = running_time/5\n",
    "print('Running time for Poly Kernel:', avg_running_time_poly,'s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBF\n",
    "\n",
    "The function for the RBF is:\n",
    "\n",
    "$$K(x_i,x_{i'})=exp(-\\gamma \\sum_{j=1}^p (x_{ij}-x_{i'j})^2$$\n",
    "\n",
    "As explained in this [Quora answer](https://www.quora.com/What-are-C-and-gamma-with-regards-to-a-support-vector-machine) $\\gamma$ controls for the influence of the support vectors on the output of the decision function, because $\\gamma$ controls the variance in the Gaussian function: lower $\\gamma$ implies large variance and therefore high influence of $x_{ij}$ on $x_{i'j}$ even when they are far away from each other.\n",
    "\n",
    "We will tune for this parameter as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:36: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 1/25\n",
      "Finished 2/25\n",
      "Finished 3/25\n",
      "Finished 4/25\n",
      "Finished 5/25\n",
      "Finished 6/25\n",
      "Finished 7/25\n",
      "Finished 8/25\n",
      "Finished 9/25\n",
      "Finished 10/25\n",
      "Finished 11/25\n",
      "Finished 12/25\n",
      "Finished 13/25\n",
      "Finished 14/25\n",
      "Finished 15/25\n",
      "Finished 16/25\n",
      "Finished 17/25\n",
      "Finished 18/25\n",
      "Finished 19/25\n",
      "Finished 20/25\n",
      "Finished 21/25\n",
      "Finished 22/25\n",
      "Finished 23/25\n",
      "Finished 24/25\n",
      "Finished 25/25\n"
     ]
    }
   ],
   "source": [
    "# validation\n",
    "kernel = 'rbf'\n",
    "random_state = 1\n",
    "best_acc = -1\n",
    "best_params = [-1,-1]  \n",
    "i = 0 \n",
    "for C in [1e-3, 1e-1,1,1e2,1e3]:\n",
    "    for gamma in [1e-3, 1e-1,1,1e2,1e3]:\n",
    "        clf = SVC(C=C, kernel=kernel, gamma=gamma, random_state=random_state)\n",
    "        clf.fit(X_train01_normalized, y_train01)\n",
    "        preds = clf.predict(X_val01_normalized)\n",
    "        acc = accuracy_score(y_val01, preds)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_params = [C, gamma]\n",
    "        i += 1\n",
    "        print('Finished %d/25'%(i))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.9946321439848437\n",
      "Best C: 100.0\n",
      "Best degree: 0.001\n"
     ]
    }
   ],
   "source": [
    "print('Best accuracy:', best_acc)\n",
    "best_C, best_gamma = best_params[0], best_params[1]\n",
    "print('Best C:', best_C)\n",
    "print('Best degree:', best_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time for RBF Kernel: 60.795526075363156 s\n"
     ]
    }
   ],
   "source": [
    "# Get training time on the best params\n",
    "running_time = 0\n",
    "for _ in range(5):\n",
    "    start = time.time()\n",
    "    clf_rbf = SVC(C=best_C, kernel=kernel, gamma=best_gamma)\n",
    "    clf_rbf.fit(X_train01_normalized, y_train01)\n",
    "    running_time = running_time + (time.time()-start)\n",
    "avg_running_time_rbf = running_time/5\n",
    "print('Running time for RBF Kernel:', avg_running_time_rbf,'s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report your training times on the dataset for the different kernels.\n",
    "\n",
    "Running time for Linear Kernel: 0.8229702663421631 s\n",
    "\n",
    "Running time for Poly Kernel: 7.8144989013671875 s\n",
    "\n",
    "Running time for RBF Kernel: 60.795526075363156 s\n",
    "\n",
    "We see that the RBF kernel takes the most time. This is because it involves more mathematic operations than the other two. For the same reason, linear kernel takes the least time to compute as it does not involve exponentials. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report your error rates on the testing dataset for the different kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error rate: 0.0009456264775413725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:36: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial)\n"
     ]
    }
   ],
   "source": [
    "# test on linear kernel\n",
    "preds_lnr = clf_lnr.predict(X_test01_normalized)\n",
    "acc_lnr = accuracy_score(preds_lnr,y_test01)\n",
    "print('Test error rate:', 1-acc_lnr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:36: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error rate: 0.0014184397163120588\n"
     ]
    }
   ],
   "source": [
    "# test on poly kernel\n",
    "preds_poly = clf_poly.predict(X_test01_normalized)\n",
    "acc_poly = accuracy_score(y_test01, preds_poly)\n",
    "print('Test error rate:', 1-acc_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:36: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error rate: 0.013238770685579215\n"
     ]
    }
   ],
   "source": [
    "# test on poly kernel\n",
    "preds_rbf = clf_rbf.predict(X_test01_normalized)\n",
    "acc_rbf = accuracy_score(y_test01, preds_rbf)\n",
    "print('Test error rate:', 1-acc_rbf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that linear kernel used in svm yields the lowest test-set error rate. This is unusual, as it is expected that non-linear kernels result in better performance (or at least equally good) than linear kernel. Two possible reasons are:\n",
    "\n",
    "1. The data is already fairly separable. This may have something to do with the two classes chosen (0 and 1), as they, from human perspective, are easily distinguishable, compared to some other non-obvious cases like 1 and 7 or 5 and 6. This explains why linear kernel alone can do such a good job.\n",
    "\n",
    "2. The above may suggest that it requires super careful tuning for the parameters of when we use polynomial or RBF kernels to find the set of parameters that beat the near perfect performance of linear kernel. In this assignment, the grid search is far from thorough and could be improved (for example, stochastic search could also be used along with grid search, in case the relationship between the model's performance and the combination of parameters doesn't follow predictable patterns as the grid search assumes.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
