{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment seeks to explore the effects of reducing and not reducing data's dimensionality on the performance of logistic regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up:\n",
    "%matplotlib inline\n",
    "import random\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from scipy.misc import imresize\n",
    "import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import time \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(**#overfitting**) To avoid overfitting, I have used the following techniques:\n",
    "\n",
    "1. Augment the data. The original data set has only 2508 samples. For each image, I cropped from it 1 or 2 pixels from all four sides and thus increase the size of the data set by 3. \n",
    "\n",
    "\n",
    "2. Keep the number of features reasonably small. It is because if the number of features is too much larger than the number of samples, the issue of overfit is more severe. I chose the image size to just $218\\times 218$, when also increased the number of data points by means of augmentation, thus reduce the problem of overfitting. I could have augment the data further to the extent that the number of samples outnumbers the number of features, but I did not due to limited computational resources (my computer collapsed when I tried augmented the data to 8-fold). \n",
    "\n",
    "\n",
    "3. Apply regularization in the cost function. Regularization has the effect of shrinking the parameters towards 0 and thus prevents learning overly complex models, whose flexibility leads to overfitting.\n",
    "\n",
    "\n",
    "4. Use cross validation to tune the parameter (the regularization strength) of the logistic regression classifier. I chose CV over the validation set approach (i.e., designating a fixed part of the training data as a validation data) because our data set is already very small. We we used the validation set approach, we will further shrink our training set, which may exacerbate the possible overfitting. We want to utilize all the training data we have. \n",
    "\n",
    "Processing code adapted from this [notebook](https://github.com/ostegm/resizing/blob/master/Resizing.ipynb?fbclid=IwAR1-jIhpdnew0hGPi1ICZTS7zcjo_JPeMUEx01VQR09NSFOb2lHNAP0_ojc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cd53fac50833>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'./images/*.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mimg_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfnames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "size = 218\n",
    "\n",
    "X_man = []\n",
    "X_woman = []\n",
    "\n",
    "for gender in ['man', 'woman']:\n",
    "    fnames = list(glob.iglob(f'./{gender}/*.JPEG'))\n",
    "    for fname in fnames:\n",
    "        img = Image.open(fname)\n",
    "        img_arr = np.array(img)\n",
    "        resized = imresize(img_arr, (size, size))\n",
    "        if resized.shape != (size, size, 3):\n",
    "            continue\n",
    "        if gender == 'man':\n",
    "            X_man.append(resized)\n",
    "        else:\n",
    "            X_woman.append(resized)\n",
    "        \n",
    "        # crop to augment\n",
    "        for crop_size in range(1,3):\n",
    "            crop_specs = (crop_size, crop_size, \n",
    "                          img.width - crop_size, img.height - crop_size)\n",
    "            crop_img = img.crop(crop_specs)\n",
    "            img_arr = np.array(crop_img)\n",
    "            resized = imresize(img_arr, (size, size))\n",
    "            if gender == 'man':\n",
    "                X_man.append(resized)\n",
    "            else:\n",
    "                X_woman.append(resized)\n",
    "        img.close()\n",
    "        \n",
    "\n",
    "X_man, X_woman = np.stack(X_man, axis=0), np.stack(X_woman, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3726, 218, 218, 3)\n",
      "(3798, 218, 218, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_man.shape)\n",
    "print(X_woman.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_man = np.array([0]*X_man.shape[0])\n",
    "y_woman = np.array([1]*X_woman.shape[0])\n",
    "y = np.concatenate((y_man, y_woman))\n",
    "X = np.concatenate((X_man, X_woman))\n",
    "X = np.reshape(X, (X.shape[0],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7524, 142572)\n",
      "(7524,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Using the Original Pixel Data\n",
    "\n",
    "We use cross validation to tune the inverse of the regularization strength (`C`). I didn't tune the penalty type and fixed it to `l1` due to reasons of computational resource. It is not that the L2 penalty involves squares and the numbers may explode (because I can always normalize the data so that they have a small range). It is that it took too long just to finish one iteration. Maybe with ambler time I will attempt tuning this (and other params) as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ...\n",
      "Finished 1/5\n",
      "Finished 2/5\n",
      "Finished 3/5\n",
      "Finished 4/5\n",
      "Finished 5/5\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "i = 0\n",
    "best_accuracy = -1\n",
    "best_C = 0\n",
    "print('Running ...')\n",
    "for C in [1/1e-3, 1/1e-1, 1, 1/1e1, 1/1e3]:\n",
    "    clf = LogisticRegression(random_state=0, \n",
    "                       penalty='l1',\n",
    "                       C=C)\n",
    "    scores = model_selection.cross_val_score(clf, X_train, y_train, cv=3)\n",
    "    if np.mean(scores) > best_accuracy:\n",
    "        best_accuracy = np.mean(scores)\n",
    "        best_C = C\n",
    "    i += 1\n",
    "    print('Finished %d/%d'%(i,5))\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Best C: 1\n",
      "Val Accuracy: 0.9097821796612936\n"
     ]
    }
   ],
   "source": [
    "print('-'*50)\n",
    "print('Best C:', best_C)\n",
    "print('Val Accuracy:', best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "clf = LogisticRegression(random_state=0, \n",
    "                       penalty='l1',\n",
    "                       C=best_C)\n",
    "clf.fit(X_train, y_train)\n",
    "end = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time running: 10.790315481026967 mins\n"
     ]
    }
   ],
   "source": [
    "print('Time running:', end/60, 'mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "preds = clf.predict(X_train)\n",
    "acc = np.sum(preds==y_train)/len(y_train)\n",
    "print('Training accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy: 0.9833887043189369\n"
     ]
    }
   ],
   "source": [
    "preds = clf.predict(X_test)\n",
    "acc = np.sum(preds==y_test)/len(y_test)\n",
    "print('Testing accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression on the Reduced Representation Created Using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract the mean from the data\n",
    "mean_X_train = np.mean(X_train, axis=0)\n",
    "X_train_pca = X_train - mean_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pca = X_test - mean_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6019, 10)\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=10)\n",
    "pca.fit(X_train_pca)\n",
    "X_r = pca.transform(X_train_pca)\n",
    "print(X_r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1,11), pca.explained_variance_ratio_)\n",
    "plt.title('Screeplot for depicting the proportion of variance explained \\n by each of the principal components')\n",
    "plt.xlabel('Principal component')\n",
    "plt.ylabel('Proportion variance explained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ISLR says that we can eye-ball the scree plot above (**#dataviz**) and look for an elbow shape to pick the number of components to use. However, it further notes that if in a supervised setting, we can always cross validate the parameter. Below I did just that, but in a sloppy way, due to (again) terribly long time it requires to train the model. It is sloppy because I used cross validation, but fit the PCA to **ALL** the training data. An ideal version would be as follows: we have k folds of data, and in each iteration, we only fit the PCA to $k-1$ fold and used the fitted results to transform the other fold and feed it to the classifer to get the validation score. \n",
    "\n",
    "Therefore, what I did below is mostly for demonstration that we could cross validate the `n_comp`, but the methodology is errored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ...\n",
      "Finished 5/50\n",
      "Finished 10/50\n",
      "Finished 15/50\n",
      "Finished 20/50\n",
      "Finished 25/50\n",
      "Finished 30/50\n",
      "Finished 35/50\n",
      "Finished 40/50\n",
      "Finished 45/50\n",
      "Finished 50/50\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "best_accuracy_pca = -1\n",
    "best_C_pca = 0\n",
    "best_n_comp = -1\n",
    "print('Running ...')\n",
    "for n_comp in range(1,11):\n",
    "    for C in [1/1e-3, 1/1e-1, 1, 1/1e1, 1/1e3]:\n",
    "        clf = LogisticRegression(random_state=0, \n",
    "                           penalty='l1',\n",
    "                           C=C)\n",
    "        X_r_best = X_r[:,:n_comp]\n",
    "        scores = model_selection.cross_val_score(clf, X_r_best, y_train, cv=10)\n",
    "        if np.mean(scores) > best_accuracy_pca:\n",
    "            best_n_comp = n_comp\n",
    "            best_accuracy_pca = np.mean(scores)\n",
    "            best_C_pca = C\n",
    "        i += 1\n",
    "        if i%5==0:\n",
    "            print('Finished %d/%d'%(i,50))\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Best number of components: 9\n",
      "Best C: 1000.0\n",
      "Val Accuracy: 0.6501110036977887\n"
     ]
    }
   ],
   "source": [
    "print('-'*50)\n",
    "print('Best number of components:', best_n_comp)\n",
    "print('Best C:', best_C_pca)\n",
    "print('Val Accuracy:', best_accuracy_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time: 0.006350040435791016 secs\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "clf = LogisticRegression(random_state=0, \n",
    "                       penalty='l1',\n",
    "                       C=best_C_pca)\n",
    "clf.fit(X_r[:,:best_n_comp], y_train)\n",
    "end = time.time() - start\n",
    "print('Running time:', end,'secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.648612726366506\n"
     ]
    }
   ],
   "source": [
    "preds = clf.predict(X_r[:,:best_n_comp])\n",
    "acc = np.sum(preds==y_train)/len(y_train)\n",
    "print('Training accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_r_test = pca.transform(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy: 0.6478405315614618\n"
     ]
    }
   ],
   "source": [
    "preds = clf.predict(X_r_test[:,:best_n_comp])\n",
    "acc = np.sum(preds==y_test)/len(y_test)\n",
    "print('Testing accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression on the Reduced Representation Created Using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6019, 1)\n"
     ]
    }
   ],
   "source": [
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "lda.fit(X_train, y_train)\n",
    "X_r_lda = lda.transform(X_train)\n",
    "print(X_r_lda.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ...\n",
      "Finished 1/5\n",
      "Finished 2/5\n",
      "Finished 3/5\n",
      "Finished 4/5\n",
      "Finished 5/5\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "best_accuracy_lda = -1\n",
    "best_C_lda = 0\n",
    "print('Running ...')\n",
    "for C in [1/1e-3, 1/1e-1, 1, 1/1e1, 1/1e3]:\n",
    "    clf = LogisticRegression(random_state=0, \n",
    "                       penalty='l1',\n",
    "                       C=C)\n",
    "    scores = model_selection.cross_val_score(clf, X_r_lda, y_train, cv=10)\n",
    "    if np.mean(scores) > best_accuracy_lda:\n",
    "        best_accuracy_lda = np.mean(scores)\n",
    "        best_C_lda = C\n",
    "    i += 1\n",
    "    print('Finished %d/%d'%(i,5))\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Best C: 10.0\n",
      "Val Accuracy: 0.9998338870431894\n"
     ]
    }
   ],
   "source": [
    "print('-'*50)\n",
    "print('Best C:', best_C_lda)\n",
    "print('Val Accuracy:', best_accuracy_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time: 0.0059049129486083984 secs\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "clf = LogisticRegression(random_state=0, \n",
    "                       penalty='l1',\n",
    "                       C=best_C_lda)\n",
    "clf.fit(X_r_lda, y_train)\n",
    "end = time.time() - start\n",
    "print('Running time:', end,'secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9996677188901811\n"
     ]
    }
   ],
   "source": [
    "preds = clf.predict(X_r_lda)\n",
    "acc = np.sum(preds==y_train)/len(y_train)\n",
    "print('Training accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_r_lda_test = lda.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy: 0.9315614617940199\n"
     ]
    }
   ],
   "source": [
    "preds = clf.predict(X_r_lda_test)\n",
    "acc = np.sum(preds==y_test)/len(y_test)\n",
    "print('Testing accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Metric \n",
    "\n",
    "I chose accuracy to evaluate the classifier's performance because:\n",
    "\n",
    "1. There is no obvious reason why one would try to avoid one type of error more than the other. In this context, we would feel as bad when we missclassify a woman's clothe as when we do a man's.\n",
    "\n",
    "2. The classes are balanced. There are 3726 samples for man clothes, and 3798 samples for woman clothes. The ratio is almost 50/50. That said, when the classifer achieve an impressive accuracy, we know that it is vertainly not due to lazily predicting everything to one class, as in cases with super imbalanced data sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting the results\n",
    "\n",
    "## Original Pixel Data\n",
    "\n",
    "The training accuracy is at a perfect level ($100\\%$). One may suspect that it is suffering heavy overfit due to the fact the number of features is much smaller than the number of observations, but the test accuracy negates that. It is nearly just as good ($98.3\\%$), suggesting no variance problem. The testing accuracy is also much better than random guessing (which would be one of $50\\%$). This is a reasonably good model on this kind of data.\n",
    "\n",
    "## Using PCA\n",
    "\n",
    "The training accuracy is much worse than that of the case above ($64.8\\%$). This is the problem of high bias. The testing accuracy is just slightly under the training accuracy ($64.7\\%$), suggesting that there is no overfit/ high variance problem. However, given that these accuracies is way lower than the optimal (Bayes) performance (the human performance), which should be very close to $100\\%$, the model suffers heavily with underfitting.\n",
    "\n",
    "One possible explanation for this poor performance is that in dimensionality reduction using PCA, it is unsupervised and thus only tries to maximize the variation in the projected data points, the goal of which may be an equivalent of having much overlap between the two classes in the reduced data space. This large inseparability may have led to the poor classfication performance. That said, we would expect a supervised dimension reduction techniques like LDA, which takes into account the labels in data, perform better.\n",
    "\n",
    "## Using LDA\n",
    "\n",
    "As expected, the training and testing accuracies are a vast improvement from those in the PCA case ($99.9\\%$ and $93.1\\%$, respectively.) The little gap between the two accuracies also shows that the model does not overfit. The little difference between the accuracies and the optimal performance ($100\\%$) shows that the model has low bias. Therefore, it is a better model than PCA, but not as good as when fit with the original data set.\n",
    "\n",
    "However, the number of components for LDA is not chosen in a proper way. Ideally, this should have also been cross validated, but I just picked $1$ because I had tried with larger quantities and the notebook collapsed. Perhaps we could have seen better performance with LDA if this had been done properly.\n",
    "\n",
    "## Synthesis\n",
    "\n",
    "No models/cases overfits.\n",
    "\n",
    "The logsitic regression used on the original (non-reduced data set) has the best performance. Its main disadvantage is the super long training time (about $10$ minutes). However, given that 1) the testing time is roughly the same (the output was printed almost instantaneously) for the three cases and 2) we don't care as much about the training time (as long as it's feasible) as we do the testing time in practice (we would be happy to incubate the model for a long period of time as long as it performs reasonable fast and accurately in testing phase), the first classifer is the most recommendable. The catch is that there is still a possible outperformance of the LDA over the original-data case if the number of components used in the LDA has been properly tuned. \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
